# ExtraÃ§Ã£o e IngestÃ£o de Dados com Embulk, Meltano e Airflow

[![Embulk](https://img.shields.io/badge/Embulk-v0.9.25-blue?logo=embulk)](https://www.embulk.org/)
[![Meltano](https://img.shields.io/badge/Meltano-Custom%20Tap-orange?logo=meltano)](https://meltano.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-12.x-blue?logo=postgresql)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-blue?logo=docker)](https://docs.docker.com/compose/)
[![Shell Script](https://img.shields.io/badge/Shell-Bash-black?logo=gnu-bash)](https://www.gnu.org/software/bash/)
[![Airflow](https://img.shields.io/badge/Airflow-2.x-blue?logo=apache-airflow)](https://airflow.apache.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

Este projeto demonstra a construÃ§Ã£o de um pipeline modularizado de ingestÃ£o de dados utilizando ferramentas modernas de engenharia de dados. A arquitetura envolve extraÃ§Ã£o de mÃºltiplas fontes (banco relacional e arquivos CSV) e carga para um banco de destino, tudo em ambiente containerizado com Docker e orquestrado com Apache Airflow.

## ğŸ”§ Tecnologias Utilizadas

- **Embulk**: extraÃ§Ã£o de dados relacionais (PostgreSQL) e carga de CSVs para PostgreSQL
- **Meltano**: extraÃ§Ã£o de arquivos CSV via tap customizado
- **PostgreSQL**: banco de origem e destino para ingestÃ£o
- **Docker Compose**: empacotamento dos serviÃ§os
- **Makefile**: automatizaÃ§Ã£o das execuÃ§Ãµes do pipeline
- **Apache Airflow**: orquestraÃ§Ã£o das tarefas
- **Shell Script**: geraÃ§Ã£o dinÃ¢mica dos jobs e controle de entrada

## ğŸ“¦ Estrutura de DiretÃ³rios

```bash
.
â”œâ”€â”€ extract-postgres-embulk/       # Container Embulk para extraÃ§Ã£o relacional
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ entrypoint.sh
â”‚   â””â”€â”€ scripts/create_ymls.sh
â”œâ”€â”€ extract-csv-meltano/           # Container Meltano com tap customizado
â”‚   â”œâ”€â”€ meltano.yml
â”‚   â””â”€â”€ plugins/extractors/tap-northwindcsv/...
â”œâ”€â”€ load-jsonl-meltano/            # Carga de JSONL via Meltano (target customizado)
â”‚   â”œâ”€â”€ entrypoint.sh
â”‚   â””â”€â”€ plugins/loaders/target-postgres-custom/...
â”œâ”€â”€ load-csv-embulk/               # Carga de CSVs no PostgreSQL via Embulk
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ entrypoint.sh
â”‚   â””â”€â”€ config/templates/base_template.yml
â”œâ”€â”€ orchestrate-airflow/           # OrquestraÃ§Ã£o com Airflow
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ dags/run_pipeline.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ northwind.sql              # Dump do banco de origem
â”‚   â”œâ”€â”€ order_details.csv          # CSV extra para ingestÃ£o via Meltano
â”‚   â””â”€â”€ postgres/                  # Dados extraÃ­dos por tabela
â”œâ”€â”€ Makefile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ docs/
    â””â”€â”€ arquitetura_de_extracao_e_ingestao.jpg
````

## ğŸ“ ConfiguraÃ§Ã£o do Ambiente

1. Copie o arquivo de variÃ¡veis de ambiente de exemplo:

```bash
cp .env.example .env
```

2. Os valores padrÃ£o jÃ¡ funcionam com os containers definidos no `docker-compose.yml`, mas vocÃª pode customizÃ¡-los conforme necessÃ¡rio.

## âœ… Etapas ConcluÃ­das

### 1. ExtraÃ§Ã£o via Embulk

* 13 tabelas do PostgreSQL (Northwind) extraÃ­das com sucesso
* Arquivos CSV organizados em `data/postgres/{tabela}/{data}/tabela.csv`
* ConfiguraÃ§Ã£o dinÃ¢mica dos YAMLs com `create_ymls.sh`

### 2. ExtraÃ§Ã£o via Meltano (CSV)

* Tap customizado Singer (`tap-northwindcsv`)
* GeraÃ§Ã£o de JSONL para ingestÃ£o padronizada

### 3. Carga via Meltano (JSONL â†’ PostgreSQL)

* Target customizado com `psycopg2`
* Pipeline Meltano funcional

### 4. Carga via Embulk (CSV â†’ PostgreSQL)

* Dockerfile com Embulk 0.9.25, plugins corretos e entrada dinÃ¢mica
* `entrypoint.sh` identifica arquivos, gera schema e executa carga
* Apaga arquivos temporÃ¡rios ao final

### 5. OrquestraÃ§Ã£o com Apache Airflow

* DAG `run_pipeline_local` orquestra extraÃ§Ã£o e carga ponta-a-ponta
* Airflow exposto via porta `8080` com interface web funcional
* Tarefas organizadas com dependÃªncias corretas

## ğŸ“Š Arquitetura do Pipeline

![Arquitetura do Pipeline](docs/arquitetura_de_extracao_e_ingestao.jpg)

## ğŸ” Comandos Ãšteis

```bash
make up                 # Sobe todos os containers
make build_all          # Rebuild completo dos serviÃ§os
make extract_all        # Executa as extraÃ§Ãµes (Embulk + Meltano)
make load_all           # Executa as cargas (JSONL + CSV)
make run_all            # Executa tudo de ponta a ponta
make reset_all          # Remove tudo e reinicia do zero
make logs_psql          # Abre o terminal no banco de origem
make logs_dest          # Abre o terminal no banco de destino
make count_tables_dest  # Lista as tabelas do banco destino
```

## âœ… ExecuÃ§Ã£o da OrquestraÃ§Ã£o com Airflow

Acesse a interface do Airflow em [http://localhost:8080](http://localhost:8080)
Login padrÃ£o: `admin` / `admin`

1. Habilite a DAG `run_pipeline_local`
2. Clique em â€œTrigger DAGâ€ para iniciar a execuÃ§Ã£o do pipeline completo

## âœï¸ Autor

Matheus Vaz â€” [@matheusvazdata](https://github.com/matheusvazdata)